{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  6\nS:  500\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v2\")\n",
    "#created the environment\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "print(\"A: \",action_size)\n",
    "print(\"S: \", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|\u001b[34;1mR\u001b[0m: | : :G|\n| : : : : |\n| : : : : |\n| | : | : |\n|\u001b[43mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table: \n[[0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n ...\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Step-1: Initializing the Q-table, load all the state values to 0. \n",
    "# Columns = Action Rows = states, i.e. \n",
    "# table = 4x16\n",
    "Q_table = np.zeros((state_size, action_size))\n",
    "print(\"Q-table: \")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating all the necessary hyperparameters:\n",
    "#learning rate, episodes, epsilon,discounts\n",
    "\n",
    "episodes = 30000\n",
    "steps_per_episodes = 100\n",
    "Learning_Rate = 0.8\n",
    "epsilon = 1.0 # probability of exploration is 1 in beginning for doing random exploration\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001 # close to 0, for exploitation\n",
    "decay_Rate = 0.005  # epsilon will decay or change downwards at this rate\n",
    "Gamma = 0.60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 6.147233333333333\n\n\nQ-table: \n[[  0.           0.           0.           0.           0.\n    0.        ]\n [ -2.26683852  -2.12731514  -2.25556128  -2.17959391  -1.870144\n  -10.773504  ]\n [ -2.22967154  -1.60617397  -1.89438024  -1.45656494  -0.7504\n  -11.14469766]\n ...\n [ -1.71392      0.41490682  -1.71392     -1.79897337 -10.23743931\n  -10.53696   ]\n [ -2.42750568  -2.27995821  -2.40605492  -2.39801168 -11.2635435\n  -10.96549073]\n [ -1.47456     -1.344       -1.4528      10.99996979  -9.984\n   -9.984     ]]\n"
     ]
    }
   ],
   "source": [
    "Rewards = [] # list of rewards\n",
    "# maximum episodes that are possible\n",
    "for episode in range(1, episodes):\n",
    "    s = env.reset()  # reset the environment after every episode\n",
    "    done = False # flag\n",
    "    Total_Reward = 0  #as per Q-table\n",
    "    \n",
    "    for step in range(1,steps_per_episodes):\n",
    "        # choose an action \"A\" in current state\n",
    "           # Chhose a random number for exploration\n",
    "        trade_off = random.uniform(0,1)            \n",
    "        \n",
    "        #if trade_off > epsilon, then do exploitation, by choosing biggest value of Q from table\n",
    "        if trade_off > epsilon:\n",
    "            action = np.argmax(Q_table[s,:]) # for that state s, provided above s = env.reset(), i.e., initial position\n",
    "        else:\n",
    "            action = env.action_space.sample() # else do the exploration\n",
    "            \n",
    "        #observe the returns of the environmetn\n",
    "        s_, r, d, i = env.step(action)\n",
    "        \n",
    "        #update Q-table\n",
    "        Q_table[s,action] = Q_table[s,action] + Learning_Rate * (r + Gamma * np.max(Q_table[s_, :]) - Q_table[s,action])\n",
    "        \n",
    "        Total_Reward += r\n",
    "        \n",
    "        s = s_ # this is the newstate\n",
    "        \n",
    "        #terminal\n",
    "        if d == True:\n",
    "            break\n",
    "        \n",
    "    #reduce epsilon because to continue for exploitation and to lessen the exploration\n",
    "    epsilon= min_epsilon + ( max_epsilon - min_epsilon) * np.exp( - decay_Rate* episode)\n",
    "    Rewards.append(Total_Reward)\n",
    "    \n",
    "print(\"Score: \" + str(sum(Rewards)/episodes))\n",
    "print(\"\\n\")\n",
    "print(\"Q-table: \")\n",
    "print(Q_table)\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\nEpisode:  1\nEpisode:  2\nEpisode:  3\nEpisode:  4\nEpisode:  5\nEpisode:  6\nEpisode:  7\nEpisode:  8\nEpisode:  9\nEpisode:  10\nEpisode:  11\nEpisode:  12\nEpisode:  13\nEpisode:  14\nEpisode:  15\nEpisode:  16\nEpisode:  17\nEpisode:  18\nEpisode:  19\nEpisode:  20\nEpisode:  21\nEpisode:  22\nEpisode:  23\nEpisode:  24\nEpisode:  25\nEpisode:  26\nEpisode:  27\nEpisode:  28\nEpisode:  29\nEpisode:  30\nEpisode:  31\nEpisode:  32\nEpisode:  33\nEpisode:  34\nEpisode:  35\nEpisode:  36\nEpisode:  37\nEpisode:  38\nEpisode:  39\nEpisode:  40\nEpisode:  41\nEpisode:  42\nEpisode:  43\nEpisode:  44\nEpisode:  45\nEpisode:  46\nEpisode:  47\nEpisode:  48\nEpisode:  49\n"
     ]
    }
   ],
   "source": [
    "#training is done \n",
    "#USe updated Q_table to play frozen lake again\n",
    "\n",
    "env.reset()\n",
    "r = []\n",
    "for episode in range(50):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    print(\"Episode: \", episode)\n",
    "    \n",
    "    for step in range(steps_per_episodes):\n",
    "        action = np.argmax(Q_table[s,:]) #take the action having maximum Q-value entry\n",
    "        s_, r, d, i = env.step(action)\n",
    "        total_reward += r\n",
    "        #print only last state of success or failure\n",
    "        if done == True:\n",
    "            r.append(total_reward)\n",
    "            env.render(mode=True)\n",
    "            print(\"Number of steps: \", step)\n",
    "            break\n",
    "        s = s_ \n",
    "env.close()        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

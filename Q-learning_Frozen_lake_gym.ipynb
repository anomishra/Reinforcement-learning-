{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "#created the environment\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  4\nS:  16\n"
     ]
    }
   ],
   "source": [
    "print(\"A: \",action_size)\n",
    "print(\"S: \", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table: \n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Step-1: Initializing the Q-table, load all the state values to 0. \n",
    "# Columns = Action Rows = states, i.e. \n",
    "# table = 4x16\n",
    "Q_table = np.zeros((state_size, action_size))\n",
    "print(\"Q-table: \")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating all the necessary hyperparameters:\n",
    "#learning rate, episodes, epsilon,discounts\n",
    "\n",
    "episodes = 10000\n",
    "steps_per_episodes = 100\n",
    "Learning_Rate = 0.8\n",
    "epsilon = 1.0 # probability of exploration is 1 in beginning for doing random exploration\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001 # close to 0, for exploitation\n",
    "decay_Rate = 0.005  # epsilon will decay or change downwards at this rate\n",
    "Gamma = 0.95 # discount rate, high because of long term reward desire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6165\n\n\nQ-table: \n[[1.32251700e-01 3.25230020e-02 3.20362997e-02 3.38002831e-02]\n [1.05970684e-02 1.03367661e-02 6.24544342e-03 2.43347480e-01]\n [5.07733282e-03 9.17826566e-03 8.99339918e-03 1.52461480e-01]\n [1.50513144e-04 9.65328874e-03 7.15934141e-03 8.19739694e-02]\n [3.00902258e-01 3.58847305e-02 7.18980540e-04 3.20935716e-02]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [3.35309494e-02 1.89079533e-06 9.08043616e-07 2.92206966e-05]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [4.28940039e-04 3.42433525e-02 4.58112717e-02 3.46640430e-01]\n [1.12013316e-03 5.79555860e-01 4.13933372e-03 3.16687571e-03]\n [2.60910902e-01 3.75249409e-03 9.38706212e-06 8.95012020e-04]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.91185529e-02 2.84738145e-02 4.29531459e-01 2.95223022e-02]\n [1.33682092e-01 8.99115486e-01 1.84311722e-01 1.27451991e-01]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "Rewards = [] # list of rewards\n",
    "# maximum episodes that are possible\n",
    "for episode in range(1, episodes):\n",
    "    s = env.reset()  # reset the environment after every episode\n",
    "    done = False # flag\n",
    "    Total_Reward = 0  #as per Q-table\n",
    "    \n",
    "    for step in range(1,steps_per_episodes):\n",
    "        # choose an action \"A\" in current state\n",
    "           # Chhose a random number for exploration\n",
    "        trade_off = random.uniform(0,1)            \n",
    "        \n",
    "        #if trade_off > epsilon, then do exploitation, by choosing biggest value of Q from table\n",
    "        if trade_off > epsilon:\n",
    "            action = np.argmax(Q_table[s,:]) # for that state s, provided above s = env.reset(), i.e., initial position\n",
    "        else:\n",
    "            action = env.action_space.sample() # else do the exploration\n",
    "            \n",
    "        #observe the returns of the environmetn\n",
    "        s_, r, d, i = env.step(action)\n",
    "        \n",
    "        #update Q-table\n",
    "        Q_table[s,action] = Q_table[s,action] + Learning_Rate * (r + Gamma * np.max(Q_table[s_, :]) - Q_table[s,action])\n",
    "        \n",
    "        Total_Reward += r\n",
    "        \n",
    "        s = s_ # this is the newstate\n",
    "        \n",
    "        #terminal\n",
    "        if d == True:\n",
    "            break\n",
    "        \n",
    "    #reduce epsilon because to continue for exploitation and to lessen the exploration\n",
    "    epsilon= min_epsilon + ( max_epsilon - min_epsilon) * np.exp( - decay_Rate* episode)\n",
    "    Rewards.append(Total_Reward)\n",
    "    \n",
    "print(\"Score: \" + str(sum(Rewards)/episodes))\n",
    "print(\"\\n\")\n",
    "print(\"Q-table: \")\n",
    "print(Q_table)\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\nEpisode:  1\nEpisode:  2\nEpisode:  3\nEpisode:  4\n"
     ]
    }
   ],
   "source": [
    "#training is done \n",
    "#USe updated Q_table to play frozen lake again\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    print(\"Episode: \", episode)\n",
    "    \n",
    "    for step in range(steps_per_episodes):\n",
    "        action = np.argmax(Q_table[s,:]) #take the action having maximum Q-value entry\n",
    "        s_, r, d, i = env.step(action)\n",
    "        \n",
    "        #print only last state of success or failure\n",
    "        if done == True:\n",
    "            env.render(mode=True)\n",
    "            print(\"Number of steps: \", step)\n",
    "            break\n",
    "        s = s_ \n",
    "env.close()        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
